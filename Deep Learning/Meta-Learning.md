
Meta-Learning: Loss function is a sum across task losses where each task loss is w.r.t test set. The training data for each task is called the "support" and the relevant test data is called the "query". To optimize this loss function, if it's differentiable then use GD, else use RL or evo approach. Iterating across all test set in tasks is called an episode. 

Model-Agnostic Meta-Learning (MAML): find init s.t. a few steps of GD on NEW task can solve the task. Let operator $U_{t}^{k}(\phi )$ be k steps of GD on $L_{t}$ with init at $\theta_{0} = \phi$, so $U_{t}^{k}(\phi) = \theta_{k}$. The optimization in MAML is then $\min_{\phi} \mathbb{E}_{t}[L_{t}(U_{t}^{k}(\phi))]$, practically, inner loop uses train set A and outer loop uses val set B: $\min_{\phi} \mathbb{E}_{t,B}[L_{t}(U_{t,A}^{k}(\phi))]$. We can optimize $\phi$ with $\frac{\partial }{\partial \phi}L_{t,B}(U_{t,A}(\phi))) = DU_{t,A}(\phi) \nabla L_{t,B}(\tilde \phi)$ where $\tilde \phi = U_{t,A}(\phi)$ and the first term is the Jacobian matrix of $U_{t,A}$. Let's look at k=1 GD step of $\phi$ : $U_{t,A}(\phi) = \phi - \epsilon \nabla L_{t,B}(\phi) \rightarrow DU_{t,a}(\phi) = I - \epsilon H(\phi)$. $g_{MAML} = \mathbb{E}_{t}\left[\frac{\partial}{\partial\phi}L_{t,B}(U_{t,A}(\phi))\right] = \mathbb{E}_{t}\left[(I - \epsilon H)\nabla L_{t}(U_{t,A}(\phi))\right]$. By a first order approximation, the first half inside the expectation is identity, so just $\mathbb{E}_{t}\left[\nabla L_{t}(U_{t,A}(\phi))\right]$, basically take the average gradient across all tasks. This 1-step approach using the expected gradient helps with computation efficiency and generalization. 

Reptile: Another MAML, For each iteration, sample task t, calculate $\tilde \phi = U_{t}^{k}(\phi)$ with optimizer, and update $\phi \leftarrow \phi + \epsilon(\tilde \phi - \phi)$. Uses first-order update (rather than 2nd order in MAML) so faster 

MAML is good bc Almost No Inner Loop due to feature reuse and optimizer can also be learned, but not preferred bc optimizer is specific to that task and not generalizable like SGD. You can also do this on NAS or data augs. 