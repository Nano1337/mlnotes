
TODO: clean up with cursor revision

(TODO: fill in missing slides here)

- Delayed Consequences: 
	- decisions now can impact things much later
	- consider benefit of decision and long-term ramifications
	- but temporal credit assignment is hard (what caused later high or low rewards)?

- Credit Assignment Problem (TODO: double check textbook for this section): 
	- given sequence of states and actions, and final sum of time-discounted future rewards, how do we infer which actions were effective at producing lots of reward and which actions were not? 
		- 
	- How do we assign credit for the observed rewards given a sequence of actions over time? 
		- need to propagate back from final state???
	- Every RL algo must include this consideration

Exploration: 
- learn about the world by making decisions
- We only get reward for decision taken, no info about other choice

Generalization: 
- policy is mapping from past experiences to action
- can't pre-program a policy bc may not be the most optimal

Example: AI Tutor as a decision process: 
- State: 
	- initial state: how much the child knows to begin with
	- states: kids state of knowledge
- Actions: 
	- next problem you give student to solve to understand student's state
- Reward model: 
	- binary, right/wrong for given problem
- Meaning of Dynamics model: 

### Markov Decision Process (MDP)

- MDP Sequence (History): $S_{0}A_{0}R_{1}S_{1}A_{1}R_{2}\dots$ 
- Markov property: 
	- $p(s_{t+1}|s_{t},a_{t},a_{t-1},\dots) = p(s_{t+1}|s_{t},a_{t})$
- Future is independent of past given present
	- only care about current and previous state to determine next state (not entire history)
	- this is a 1st order markov
- Defines agent-environment interactions at discrete time steps

**Finite MDP**: 
- Finite MDP (S,A,R) are finite
- $R_{t}$ and $S_{t}$ are discrete probability distributions
	- $p(s_{t}|s_{t-1},a)$
	- $p(r_{t}|s_{t-1}, a)$
- Next state is then determined by: 
	$$
	\begin{align}
	p(s',r|s,a) = P(S_{t}=s', R_{t}=r | S_{t-1}=s,A_{t-1}=a) \\
	s',s \in S, r\in R, a\in A(s)
	\end{align}
	$$
- Number of state spaces are finite, continuous MDP have infinite state spaces

**Markov Assumption**
- Hypertension control: 
	- state is current blood pressure and action is whether to take medication or not
	- Example: treating diabetes, need to track history of states
	- System is not Markov in the real world
- Website shopping: 
	- state is current product viewed by customer and action is what other product to recommend
	- Not exactly, recsys works with history and general profile of user/activity for recs. 

