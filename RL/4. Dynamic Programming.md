**Overview**: 
- methods that can compute optimal policies
- Assumption: perfect model of env as MDP
	- limited utility bc of this and is memory intensive practically bc of caching

### Policy Evaluation (Prediction)
- TL;DR: Iterative Estimation of Value Function
- Problem setting: compute $v_{\pi}$ for arbitrary $\pi$ 
	- Recall that existence and uniqueness of $v_{\pi}$ are guaranteed as long as either $\gamma < 1$ or eventual termination guaranteed from all states under $\pi$ (episodic)
	- If env dynamics completely known, then calc $v_{\pi}$ is system of $|S|$ linear eqns in $|S|$ unknowns ($v_{\pi}(s), s\in S$) 
- While we can solve the linear system via matrix inversion, this isn't scalable, so we solve by iteration (same reason why gradient descent exists)
- **Iterative Policy Evaluation**: Bellman equation for $v_{\pi}$ as update rule: 
  $$
  v_{k+1} = \sum\limits_{a}\pi(a|s)\sum\limits_{s',r}p(s',r|s,a)
  [r+\gamma v_{k}(s')]$$
	- Pick random state for $v_{0}$ (except for terminal state, which has value 0). Can randomly init values for all states.
	- Update each state given all its adjacent successor state values. This is called "expected return". One iteration is called a "sweep"
	- Repeat process until convergence. Sequence converges to $v_{\pi}$ as $k \rightarrow \infty$ under same conds that guarantees existence of $v_{\pi}$ ($\gamma$ or episodic)
- Practical implementation: 
	- Can use 2 arrays, one of original values and one with updated values. Alternatively, you can update values in place (saves memory). This in-place iteration usually is faster to converge bc you get new intermediate values of a single sweep immediately
- Algorithm: 
  ![[Pasted image 20240908182528.png]]
	- $\theta$ is threshold for delta required for convergence condition to be met. Iteration until $\infty$ would have $\theta=0$. We denote value function with $V(s)$ instead to indicate that it's an approximation 
- Example: 
	- Gridworld 4x4 with states numbered ${1\dots 14}$ with first and last state as terminal states. Reward is -1 unless terminal=0. Equiprobable random policy and deterministic transitions, which means $p(s',r|s,a)=1$ if it exists.
	- ![[Pasted image 20240908185100.png]]
	- What is $q_{\pi}(11, down)$? This would end up in terminal state so R=0 with deterministic transition so action-value is 0
	- What is $q_{\pi}(7, down)$? This would be $q_{\pi}(7, down)=p(11,-1|7, down)[-1 + \gamma q_{\pi}(11, down)]$ which evaluates to -1
### Policy Improvement
- TL;DR: After estimating value function for every state, use greedy policy on state values
- Problem Setting: 
	- Once we have $v_{\pi}$ for arbitrary $\pi$ , for some state s we want to know whether to change the policy. 
	- We can determine this by simply choosing an action and seeing if the value of that action is better than the value of following the existing policy. 
	- More formally, this is the *policy improvement theorem*. Let $\pi$ and $\pi^{'}$ be a pair of deterministic policies s.t. $\forall s\in S$: 
	  $$
	  q_{\pi}(s, \pi^{'}(s)) \geq v_{\pi}(s)
	  $$
		- Then policy $\pi^{'}$ is at least as good as $\pi$. If we adopt $\pi^{'}$, then: 
		    $$
		    v_{\pi'}(s) \geq v_{\pi}(s) 
		    $$
	- One policy that meets these criteria is the greedy policy: 
	  $$
	  \pi^{'}(s) = arg\max_a q_{\pi}(s,a)
	  $$
	- Policy improvement: process of making new policy that improves on original policy by making it greedy w.r.t value function of original policy

### Policy Iteration: 
- TL;DR: Flip between policy evaluation and improvement until convergence bc value functions change based on changing policies and vice versa
- 