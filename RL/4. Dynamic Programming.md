**Overview**: 
- methods that can compute optimal policies
- Assumption: perfect model of env as MDP
	- limited utility bc of this and is memory intensive practically bc of caching

### Policy Evaluation (Prediction)
- TL;DR: Iterative Estimation of Value Function
- Problem setting: compute $v_{\pi}$ for arbitrary $\pi$ 
	- Recall that existence and uniqueness of $v_{\pi}$ are guaranteed as long as either $\gamma < 1$ or eventual termination guaranteed from all states under $\pi$ (episodic)
	- If env dynamics completely known, then calc $v_{\pi}$ is system of $|S|$ linear eqns in $|S|$ unknowns ($v_{\pi}(s), s\in S$) 
- While we can solve the linear system via matrix inversion, this isn't scalable, so we solve by iteration (same reason why gradient descent exists)
- **Iterative Policy Evaluation**: Bellman equation for $v_{\pi}$ as update rule: 
  $$
  v_{k+1} = \sum\limits_{a}\pi(a|s)\sum\limits_{s',r}p(s',r|s,a)
  [r+\gamma v_{k}(s')]$$
	- Pick random state for $v_{0}$ (except for terminal state, which has value 0). Can randomly init values for all states.
	- Update each state given all its adjacent successor state values. This is called "expected return". One iteration is called a "sweep"
	- Repeat process until convergence. Sequence converges to $v_{\pi}$ as $k \rightarrow \infty$ under same conds that guarantees existence of $v_{\pi}$ ($\gamma$ or episodic)
- Practical implementation: 
	- Can use 2 arrays, one of original values and one with updated values. Alternatively, you can update values in place (saves memory). This in-place iteration usually is faster to converge bc you get new intermediate values of a single sweep immediately
- Algorithm: 
  ![[Pasted image 20240908182528.png]]
	- $\theta$ is threshold for delta required for convergence condition to be met. Iteration until $\infty$ would have $\theta=0$. We denote value function with $V(s)$ instead to indicate that it's an approximation 
- Example: 
	- Gridworld 4x4 with states numbered ${1\dots 14}$ with first and last state as terminal states. Reward is -1 unless terminal=0. Equiprobable random policy and deterministic transitions, which means $p(s',r|s,a)=1$ if it exists.
	- ![[Pasted image 20240908185100.png]]
	- What is $q_{\pi}(11, down)$? This would end up in terminal state so R=0 with deterministic transition so action-value is 0
	- What is $q_{\pi}(7, down)$? This would be $q_{\pi}(7, down)=p(11,-1|7, down)[-1 + \gamma q_{\pi}(11, down)]$ which evaluates to -1
### Policy Improvement
- TL;DR: After estimating value function for every state, use greedy policy on state values
- Problem Setting: 
	- Once we have $v_{\pi}$ for arbitrary $\pi$ , for some state s we want to know whether to change the policy. 
	- We can determine this by simply choosing an action and seeing if the value of that action is better than the value of following the existing policy. 
	- More formally, this is the *policy improvement theorem*. Let $\pi$ and $\pi^{'}$ be a pair of deterministic policies s.t. $\forall s\in S$: 
	  $$
	  q_{\pi}(s, \pi^{'}(s)) \geq v_{\pi}(s)
	  $$
		- Then policy $\pi^{'}$ is at least as good as $\pi$. If we adopt $\pi^{'}$, then: 
		    $$
		    v_{\pi'}(s) \geq v_{\pi}(s) 
		    $$
	- One policy that meets these criteria is the greedy policy: 
	  $$
	  \pi^{'}(s) = arg\max_a q_{\pi}(s,a)
	  $$
	- Policy improvement: process of making new policy that improves on original policy by making it greedy w.r.t value function of original policy

### Policy Iteration: 
- TL;DR: Alternate between policy evaluation and improvement until convergence bc value functions change based on changing policies and vice versa
- Alternating between better policies that are used for better optimal value estimates gives monotonically improving results. This alternating process is called policy iteration
- Algorithm: 
  ![[Pasted image 20240909011531.png]]
	- For policy evaluation, value fxns are init with value fxns from previous iteration bc value fxns don't change much and would lead to much faster convergence
	- Iteration algo stops when policy doesn't change in improvement else if a more optimal action is taken given a particular state, then do policy eval again. 

### Value Iteration 
- TL;DR: policy eval (one sweep) -> policy improvement (max action)
- Problem Setting: 
	- Policy Iteration requires repeated Policy evaluation, which in of itself requires iteration to the limit until convergence. Can we stop before convergence?
	- Value Iteration is the special case when policy evaluation is stopped just after one sweep
- This update operation is policy improvement + one sweep policy eval: 
  $$
  v_{k+1} = \max_{a}\sum\limits_{s',r}p(s',r|s,a)[r+\gamma v_{k}(s')]
  $$
  For arbitrary $v_{0}$, it's shown to converge to $v_{*}$
- Algorithm: 
  ![[Pasted image 20240909225906.png]]
	- policy eval (one sweep) -> policy improvement (max action)

#### Async DP: 
- Problem setting: 
	- single sweeps are expensive if state set is very large
	- Just update value of state in any order (in-place)
- Makes it easier to intermix computation with real-time interaction
	- to solve MDP, use iterative DP while agent is experiencing MDP to focus updates on parts of state set that are most relevant to agent

### Generalized Policy Iteration:
- refers to letting policy eval and improvement processes interact, independent of details 
- eval and improvement can be seen as adversarial training. For example, making policy greedy in improvement makes value function incorrect for changed policy and causes policy to no longer be greedy. These eventually converge to optimal value function and optimal policy