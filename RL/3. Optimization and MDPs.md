
After taking notes, watch that excellent YouTube lecture (and send to Sid)

Goal: Either estimate $q_{*}(s, a)$ or $v_{*}(s)$ 

Lecture notes I need to clean up lol
- Delayed Consequences: 
	- decisions now can impact things much later
	- consider benefit of decision and long-term ramifications
	- but temporal credit assignment is hard (what caused later high or low rewards)?

- Credit Assignment Problem (TODO: double check textbook for this section): 
	- given sequence of states and actions, and final sum of time-discounted future rewards, how do we infer which actions were effective at producing lots of reward and which actions were not? 
		- 
	- How do we assign credit for the observed rewards given a sequence of actions over time? 
		- need to propagate back from final state???
	- Every RL algo must include this consideration

Exploration: 
- learn about the world by making decisions
- We only get reward for decision taken, no info about other choice

Generalization: 
- policy is mapping from past experiences to action
- can't pre-program a policy bc may not be the most optimal

Example: AI Tutor as a decision process: 
- State: 
	- initial state: how much the child knows to begin with
	- states: kids state of knowledge
- Actions: 
	- next problem you give student to solve to understand student's state
- Reward model: 
	- binary, right/wrong for given problem
- Meaning of Dynamics model: 

### Markov Decision Process (MDP)

**MDP Model**
![[Pasted image 20240901120059.png]]
- Notation: 
	- discrete time steps, $t = 0, 1, 2, 3, \dots$ 
	- agent has some representation of env state, $S_{t} \in \mathcal{S}$ 
	- Actions possible at given state, $A_{t} \in \mathcal{A(s)}$ 
	- As a consequence of action, scalar reward is received, $R_{t+1} \in \mathcal{R} \subset \mathbb{R}$  
- MDP Sequence (History): $S_{0}A_{0}R_{1}S_{1}A_{1}R_{2}\dots$ 
- Markov property (Transition/dynamics model): 
	- the single state must include info about all aspects of past agent-env interaction that make a difference for a future (this is an aggressive assumption) 
	- $p(s_{t+1}|s_{t},a_{t},a_{t-1},\dots) = p(s_{t+1}|s_{t},a_{t})$
- Future is independent of past given present
	- only care about current and previous state to determine next state (not entire history)
	- this is a 1st order markov
- Defines agent-environment interactions at discrete time steps

**Finite MDP**: 
- Finite MDP (S,A,R) are finite
- $R_{t}$ and $S_{t}$ are discrete probability distributions
	- $p(s_{t}|s_{t-1},a)$
	- $p(r_{t}|s_{t-1}, a)$
- Next state is then determined (Dynamics model): 
	$$
	\begin{align}
	p(s',r|s,a) = P(S_{t}=s', R_{t}=r | S_{t-1}=s,A_{t-1}=a) \\
	s',s \in S, r\in R, a\in A(s) \\
	p : \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,1]
	\end{align}
	$$
- And because the probabilities given by $p$ completely characterize the finite env's dynamics, the normalization condition states that: 
	$$
	\begin{align}
	\sum\limits_{s'\in \mathcal{S}}\sum\limits_{r \in \mathcal{R}} p(s',r|s,a)=1, \forall s\in \mathcal{S},a\in \mathcal{A}(s)
	\end{align}
	$$
- Following, we can derive the state-transition probabilities by marginalizing over the reward: $p:\mathcal{S} \times \mathcal{S} \times \mathcal{A}$ 
	$$
p(s'|s,a) = Pr\{S_{t}=s'|S_{t-1}=s,A_{t-1}=a\} = \sum\limits_{r \in \mathcal{R}} p(s',r|s,a)
$$
- And the expected reward for state-action pairs: $p : \mathcal{S} \times \mathcal{A}$ 
	- We can first marginalize over next states and then take the expectation over rewards: 
		$$
		r(s,a) = \sum\limits_{r\in \mathcal{R}}r\sum\limits_{s'\in \mathcal{S}} p(s',r|s,a)
		$$
- And also the expected reward for the state-action-next-state triplets too! $r:\mathcal{S} \times \mathcal{A} \times \mathcal{S}$ 
	$$
	r(s',a,s) = \sum\limits_{r\in \mathcal{R}}r\frac{p(s',r|s,a)}{p(s'|s,a)} 
	$$
	- Derivation: 
		- Write out expectation: $r(s',a,s)=\sum\limits_{r\in \mathcal{R}}r*p(r|s',a,s)$  
		- Focusing on latter term, apply P(A|B) conditional prob rule: 
		  $$
		  p(r|s',a,s)=\frac{p(s',r,s,a)}{p(s',s,a)}
		  $$
		- Apply joint probability rule and simplify: 
		  $$
		  = \frac{p(r,s'|s,a)p(s,a)}{p(s'|s,a)p(s,a)} = \frac{p(r,s'|s,a)}{p(s'|s,a)}
		  $$
- Other facts: 
	- Number of state spaces are finite, continuous MDP have infinite state spaces
	- The sum of transition probabilities from a given state  s  and action  a  to all possible next states  s'  must always sum to 1. This is because the system must transition to exactly one next state.
	  $$
	  \sum\limits_{s'\in \mathcal{S}}p(s'|s,a) = 1
	  $$

**Markov Property Assumption doesn't always hold**
- Hypertension control: 
	- state is current blood pressure and action is whether to take medication or not
	- Example: treating diabetes, need to track history of states
	- System is not Markov in the real world
- Website shopping: 
	- state is current product viewed by customer and action is what other product to recommend
	- Not exactly, recsys works with history and general profile of user/activity for recs. 

**Types of Sequential Decision Processes**
- Is the state Markov? Is the world partially observable (POMDP)
- Are dynamics deterministic or stochastic (probabilistic)
- Do actions influence only immediate next state or future states as well? 

### Example: Recycling Robot: 
- Actions: 
	1. active search for can to recycle
	2. wait for someone to bring it can
	3. back to home base and recharge
- Decisions: 
	- Based on energy level of battery
		- A(high) = {search, wait}
		- A(low) = {search, wait, recharge}
![[Pasted image 20240901130154.png]]
Personal Notes: 
- $\beta$ here should a value close to 1
	- This is because 1-$\beta$ should be low since the probability of (low, search, high) should be very low

**Goals and Rewards**
- Reward Hypothesis: 
	- achieving a goal is equivalent to maximizing the cum sum of received signal (i.e. reward), as opposed to immediate reward
- Makes RL flexible and widely applicable. Creating this reward function properly is key. The reward signal communicates "what" how want achieved, not "how" (we can't naively instill priors/human strategies)

### Returns and Episodes
- Expected Return (simplest case): $G_{t}=R_{t+1}+R_{t+2}+\dots+R_{T}$ where T is final time step. This makes sense for episodes, e.g. plays of game, trips through maze. Each episode ends in terminal state -> reset. Episodes are independent from others. 
	- Notation: set of nonterminal states: $\mathcal{S}$ and with terminal state: $\mathcal{S}^{+}$. Time of termination $T$ is an RV that usually varies per episode
- Continuing tasks: 
	- can't break into episodes, continues without limit. Example: robot with long life span. Can't use simple expected return formula bc $T=\infty$ and return could also be inf. 
- Expected Discounted Return: 
  $$
  G_{t}=R_{t+1}+\gamma R_{t+2}+ \gamma^{2}R_{t+3}+\dots+\sum\limits_{k=0}^{\infty}\gamma^{k}R_{t+k+1}
  $$ where $\gamma \in [0,1]$ is the discount rate. 
	- if $\gamma=0$, then agent is "myopic" and greedy to maximize immediate reward
	- As $\gamma \rightarrow 1$ then future rewards are more accounted for, agent is farsighted
	- This can be rewritten as a recursive relation: 
	  $$
	  G_{t}=R_{t+1}+\gamma G_{t+1}
	  $$
	- and also restated as an infinite geometric series, which has a closed form given that reward is constant R and $\gamma < 1$ 
	  $$
	  G_{t}=R\sum\limits_{k=0}^{\infty}\gamma^{k}=\frac{R}{1-\gamma}
	  $$
- Example: Pole-Balancing: 
	- natural episodes are repeated attempts to balance the pole
	- If we use reward +1 for every time step on no failure, then balancing forever would be reward of $\infty$, which is not desired. Can rewrite as -1 reward for fail and 0 for all other times. The reward at each time would be $-\gamma^{K-1}$ where K is number of time steps until failure and $\gamma$ is discount factor. Note that while episodes are independent from each other, we can have $\gamma$ for each time step within an episode as this is considered a continuing task if the pole balancer goes to $T=\infty$. 
	- This would be different if we used discounted, continuing formulation as we would have another additional discount factor as part of the expected return since we don't reset to 0 upon failure. 

**Unifying Notation**
- Episodes have their own time steps, so we would have two params $t,i$ as state representation at time $t$ for episode $i$ but in practice, we don't refer specifically to an episode so we abuse notation by only keeping time step $t$. 
- Since episodic has finite T while continuing has infinite T, the only way to unify is to introduce absorbing state that infinitely repeats R=0 for episodic case![[Pasted image 20240901142012.png]]
- The unified return is seen below. It allows for $T=\infty$ for continuing case and $\gamma=1$ for episodic case bc defined (i.e. no discounting). These are mutually exclusive (bc $\gamma \neq 1$ for $T=\infty$ case). 
  $$
  G_{t}=\sum\limits_{k=t+1}^{T}\gamma^{k-t-1}R_{k}
  $$ 



**Episodic Tasks**
- Episode starts at state $S_{t}$ and terminates after a finite number of steps
- There's terminal state (e.g. board games) => goal/loss state
- Can do this multiple times, each seq we generate is called an episode
	- add notation here

**Continuing Tasks**
- No finite end, but agent-env interactions go on forever 
	- building control; cart-pole system
- Here, reward computation needs discounting
	- Discount factory $\gamma$ 
	- $G_{t}= R_{t+1} + \gamma R_{t+2}+ \gamma^{2} R_{t+3} + \dots = \sum\limits^{\infty}_{k=0}R_{t+k+1}$
	- $\gamma$ = 0, agent is myopic
	- 0 < $\gamma$ < 1; reward converges to a finite value, if indiv rewards are finite
	- $G_{t}= R_{t+1} + \gamma$. # TODO: add the rest here
- Cart-Pole Balance: 
	- Goal: keep pole as close to vertical (hinged to cart)
	- Reward: +1 if pole hinged; 0 if pole falls
	- maximize reward with discount factor