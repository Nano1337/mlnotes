
After taking notes, watch that excellent YouTube lecture (and send to Sid)

Goal: Either estimate $q_{*}(s, a)$ or $v_{*}(s)$ 

Lecture notes I need to clean up lol
- Delayed Consequences: 
	- decisions now can impact things much later
	- consider benefit of decision and long-term ramifications
	- but temporal credit assignment is hard (what caused later high or low rewards)?

- Credit Assignment Problem (TODO: double check textbook for this section): 
	- given sequence of states and actions, and final sum of time-discounted future rewards, how do we infer which actions were effective at producing lots of reward and which actions were not? 
		- 
	- How do we assign credit for the observed rewards given a sequence of actions over time? 
		- need to propagate back from final state???
	- Every RL algo must include this consideration

Exploration: 
- learn about the world by making decisions
- We only get reward for decision taken, no info about other choice

Generalization: 
- policy is mapping from past experiences to action
- can't pre-program a policy bc may not be the most optimal

Example: AI Tutor as a decision process: 
- State: 
	- initial state: how much the child knows to begin with
	- states: kids state of knowledge
- Actions: 
	- next problem you give student to solve to understand student's state
- Reward model: 
	- binary, right/wrong for given problem
- Meaning of Dynamics model: 

### Markov Decision Process (MDP)

**MDP Model**
![[Pasted image 20240901120059.png]]
- Notation: 
	- discrete time steps, $t = 0, 1, 2, 3, \dots$ 
	- agent has some representation of env state, $S_{t} \in \mathcal{S}$ 
	- Actions possible at given state, $A_{t} \in \mathcal{A(s)}$ 
	- As a consequence of action, scalar reward is received, $R_{t+1} \in \mathcal{R} \subset \mathbb{R}$  
- MDP Sequence (History): $S_{0}A_{0}R_{1}S_{1}A_{1}R_{2}\dots$ 
- Markov property (Transition/dynamics model): 
	- the single state must include info about all aspects of past agent-env interaction that make a difference for a future (this is an aggressive assumption) 
	- $p(s_{t+1}|s_{t},a_{t},a_{t-1},\dots) = p(s_{t+1}|s_{t},a_{t})$
	- TODO: add reward model here as well
- Future is independent of past given present
	- only care about current and previous state to determine next state (not entire history)
	- this is a 1st order markov
- Defines agent-environment interactions at discrete time steps

**Finite MDP**: 
- Finite MDP (S,A,R) are finite
- $R_{t}$ and $S_{t}$ are discrete probability distributions
	- $p(s_{t}|s_{t-1},a)$
	- $p(r_{t}|s_{t-1}, a)$
- Next state is then determined (Dynamics model): 
	$$
	\begin{align}
	p(s',r|s,a) = P(S_{t}=s', R_{t}=r | S_{t-1}=s,A_{t-1}=a) \\
	s',s \in S, r\in R, a\in A(s) \\
	p : \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,1]
	\end{align}
	$$
- And because the probabilities given by $p$ completely characterize the finite env's dynamics, the normalization condition states that: 
	$$
	\begin{align}
	\sum\limits_{s'\in \mathcal{S}}\sum\limits_{r \in \mathcal{R}} p(s',r|s,a)=1, \forall s\in \mathcal{S},a\in \mathcal{A}(s)
	\end{align}
	$$
- Following, we can derive the state-transition probabilities by marginalizing over the reward: $p:\mathcal{S} \times \mathcal{S} \times \mathcal{A}$ 
	$$
p(s'|s,a) = Pr\{S_{t}=s'|S_{t-1}=s,A_{t-1}=a\} = \sum\limits_{r \in \mathcal{R}} p(s',r|s,a)
$$
- And the expected reward for state-action pairs: $p : \mathcal{S} \times \mathcal{A}$ 
	- We can first marginalize over next states and then take the expectation over rewards: 
		$$
		r(s,a) = \sum\limits_{r\in \mathcal{R}}r\sum\limits_{s'\in \mathcal{S}} p(s',r|s,a)
		$$
- And also the expected reward for the state-action-next-state triplets too! $r:\mathcal{S} \times \mathcal{A} \times \mathcal{S}$ 
	$$
	r(s',a,s) = \sum\limits_{r\in \mathcal{R}}r\frac{p(s',r|s,a)}{p(s'|s,a)} 
	$$
	- Derivation: 
		- Write out expectation: $r(s',a,s)=\sum\limits_{r\in \mathcal{R}}r*p(r|s',a,s)$  
		- Focusing on latter term, apply P(A|B) conditional prob rule: 
		  $$
		  p(r|s',a,s)=\frac{p(s',r,s,a)}{p(s',s,a)}
		  $$
		- Apply joint probability rule and simplify: 
		  $$
		  = \frac{p(r,s'|s,a)p(s,a)}{p(s'|s,a)p(s,a)} = \frac{p(r,s'|s,a)}{p(s'|s,a)}
		  $$
- Other facts: 
	- Number of state spaces are finite, continuous MDP have infinite state spaces
	- The sum of transition probabilities from a given state  s  and action  a  to all possible next states  s'  must always sum to 1. This is because the system must transition to exactly one next state.
	  $$
	  \sum\limits_{s'\in \mathcal{S}}p(s'|s,a) = 1
	  $$

**Markov Property Assumption doesn't always hold**
- Hypertension control: 
	- state is current blood pressure and action is whether to take medication or not
	- Example: treating diabetes, need to track history of states
	- System is not Markov in the real world
- Website shopping: 
	- state is current product viewed by customer and action is what other product to recommend
	- Not exactly, recsys works with history and general profile of user/activity for recs. 

**Types of Sequential Decision Processes**
- Is the state Markov? Is the world partially observable (POMDP)
- Are dynamics deterministic or stochastic (probabilistic)
- Do actions influence only immediate next state or future states as well? 

TODO: add in MDP page full of formulas and understand


### Example: Recycling Robot: 
- Actions: 
	1. active search for can to recycle
	2. wait for someone to bring it can
	3. back to home base and recharge
- Decisions: 
	- Based on energy level of battery
		- A(high) = {search, wait}
		- A(low) = {search, wait, recharge}
![[Pasted image 20240901130154.png]]
Personal Notes: 
- $\beta$ here should a value close to 1
	- This is because 1-$\beta$ should be low since the probability of (low, search, high) should be very low

**Goals and Rewards**
- Reward Hypothesis: 
	- achieving a goal is equivalent to maxing the sepected vlaue of cum sum of received signal (i.e. reward)
- Makes RL flexible and widely applicable. Creating this reward function properly is key

TODO: add "summary of MDP" slide to organize notes

**Time Discounting**
????

**Episodic Tasks**
- Episode starts at state $S_{t}$ and terminates after a finite number of steps
- There's terminal state (e.g. board games) => goal/loss state
- Can do this multiple times, each seq we generate is called an episode
	- add notation here

**Continuing Tasks**
- No finite end, but agent-env interactions go on forever 
	- building control; cart-pole system
- Here, reward computation needs discounting
	- Discount factory $\gamma$ 
	- $G_{t}= R_{t+1} + \gamma R_{t+2}+ \gamma^{2} R_{t+3} + \dots = \sum\limits^{\infty}_{k=0}R_{t+k+1}$
	- $\gamma$ = 0, agent is myopic
	- 0 < $\gamma$ < 1; reward converges to a finite value, if indiv rewards are finite
	- $G_{t}= R_{t+1} + \gamma$. # TODO: add the rest here
- Cart-Pole Balance: 
	- Goal: keep pole as close to vertical (hinged to cart)
	- Reward: +1 if pole hinged; 0 if pole falls
	- maximize reward with discount factor