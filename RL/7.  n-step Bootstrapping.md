- Interpolation between MC (episode-level) and TD (step-level) methods
- enable bootstrapping over multiple timesteps

### n-step TD Prediction 
- Example: 2-step update based on first 2 rewards and estimate of state two steps later
- Let target T be last time step of episode
- Recall one-step return defined as: 
  $$
  G_{t:t+1}=R_{t+1} + \gamma V_{t}(S_{t+1})
  $$
- Accounting for geometric series of discounting, n-step return: 
  $$
  G_{t:t+n}= R_{t+1}+\gamma R_{t+2} + \dots + \gamma^{n-1}R_{t+n}+\gamma^{n}V_{t+n-1}(S_{t+n}), \forall n \geq 1, 0 \leq t < T-n
  $$
  - If $t+n \geq T$ then missing terms are 0 and n-step return is equal to ordinary full return ($G_{t}$) 
  - Practically tho, n-step returns for n>1 involve future rewards/states that's not available at transition time t:t+1. The first time these are available is t+n (basically only after sliding window of size n can we calculate value). This follows: 
    $$
    V_{t+n}(S_{t}) = V_{t+n-1}(S_{t}) + \alpha[G_{t:t+n}-V_{t+n-1}(S_{t})], 0 \leq t < T
    $$
    ![[Pasted image 20240917231731.png]]
    - $\tau \geq 0$ here is the point at which we can start running value calculations since we are past the window of n-steps and have enough expected return steps 
  - Bc the n-step return uses the value function $V_{t+n-1}$ to correct for the missing rewards beyond $R_{t+n}$ , which means that the expectation of the n-step return is a better estimate of $v_{\pi}$ (lower error) than $V_{t+n-1}$. This is called the *error-reduction property* and guarantees convergence
    $$
    \max_{s}|\mathbb{E}_{\pi}[G_{t:t+n}-v_{\pi}(s)]| \leq \gamma^{n}\max_{s}|V_{t+n-1}-v_{\pi}(s)|, n\geq1
    $$ 
### n-step Sarsa
![[Pasted image 20240917233803.png]]
- Main changes: use state-action pairs instead of just actions with $\epsilon$-greedy policy
  $$
  Q_{t+n}(S_{t},A_{t})=Q_{t+n-1}(S_{t},A_{t}) + \alpha [G_{t:t+n}-Q_{t+n-1}(S_{t},A_{t})]
  $$
  ![[Pasted image 20240917234213.png]]
  - From the gridworld example, the first grid is an example episode, second grid is showing that only the final action was strengthened while the 10-step strengthened the last 10 actions (which would lead to much faster convergence)
  - N-step expected Sarsa: just like n-step Sarsa but last element is branch over all action possibilities weighted. If s is terminal, then expected approx value is 0. 
    $$
    \begin{align}
    G_{t:t+n}= R_{t+1}+\dots +\gamma^{n-1}\bar{V}_{t+n-1}(S_{t+n}) \\
    \bar V_{t}(s) = \sum\limits_{a} \pi(a|s)Q_{t}(s,a), \forall s\in S
    \end{align}
    $$
    
### n-step Off-policy Learning
- Recall off-policy TD using target policy $\pi$ as greedy and behavioral policy $b$ as $\epsilon$-greedy. The update is: 
  $$
  \begin{align}
  V_{t+n}(S_{t}) = V_{t+n-1}(S_{t}) + \alpha \rho_{t:t+n-1}[G_{t:t+n}-V_{t+n-1}(S_{t})] \\
  \rho_{t:h}= \Pi_{k=t}^{min(h,T-1)}\frac{\pi(A_{k}|S_{k})}{b(A_{k}|S_{k})}
  \end{align}
  $$
	- Interpreting this formula in cases: 
		- $\pi << b$ : n-step return given near 0 weight and ignored
		- $\pi >> b$ overweight target policy (bc that's what we want)
		- $\pi == b$ : on-policy case, importance sampling ratio is 1
The off-policy form of n-step Sarsa: 
$$
Q_{t+n}(S_{t},A_{t})=Q_{t+n-1}(S_{t},A_{t})+\alpha \rho_{t+1:t+n}[G_{t:t+n}-Q_{t+n-1}(S_{t},A_{t})]
$$
![[Pasted image 20240918000813.png]]

### Off-policy w/o Importance Sampling: n-step Tree Backup Algo
- TLDR: also estimate action values for actions that weren't selected through bootstrapping (i.e. the leaf nodes)
- Equation: 
  $$
  Q_{t+n}(S_{t},A_{t}) = Q_{t+n-1}(S_{t},A_{t}) + \alpha [G_{t:t+n}-Q_{t+n-1}(S_{t}, A_{t})]
  $$
  ![[Pasted image 20240923143400.png]]
