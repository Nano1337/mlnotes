### Overview: 
- MC Methods don't assume complete knowledge of env (no need for knowledge of dynamics) - require only experience: sample seq of S,A,R with env
- Assumptions: 
	- episodic
	- only on episode completion do value estimates and policy change
	- method based on averaging complete returns

### MC Prediction:
- Recall value of a state is expected return of future states. One way to estimate this is to average returns observed after visits to that state. 
	- As more returns are observed, then average should converge to expected value 
	- Don't bootstrap as previously defined bc estimates for each state are independent (unlike DP which convergence relies on previously cached calcs)
- Types of visits: each occurrence of state $s$ in episode is "visit to s"
	- first-visit MC: 
		- estimates $v_{\pi}(s)$ as average of returns following all visits to s (deduplicate multiple occurrences of s and only keep first visit per episode)
		- Each return is IID estimate of $v_{\pi}(s)$ with finite variance. By law of large numbers, this converges to expected value and mean is unbiased estimate. Standard dev of error is $1/\sqrt{n}$ where n is num returns averaged 
		  ![[Pasted image 20240910110405.png]]
	- every-visit MC: 
		- average returns following all visits to s 
		- estimates converge quadratically to $v_{\pi}(s)$

### Examples
- Blackjack is formulated as episodic finite MDP but state space is massive so infeasible to conduct state sweep for DP. Learn from experience instead. 
- MC methods estimate value of single state independent of number of states (i.e. don't scale with num states) so very cheap to generate many sample episodes starting from states of interest and only averaging returns from those states
	- In other words, can select states that you are seeing returns from, don't have to enumerate sweep across all possible states (many of which can be irrelevant). This allows injection of priors. 

### MC Estimation of Action Values
- if model not available, then very useful to estimate action values too
- state action pair is visited in an episode if ever the state s is visited and action a is taken 
- both first and every visit converge quadratically as num visits to each state-action pair approaches inf
- problem is that many state-action pairs are never visited, especially for deterministic policy (will only observed returns for one of actions from each state). The solution would be estimating value of all actions from each state, not just the one we currently favor
	- Falls under general problem of maintaining exploration
	- Concrete solution is to specify episodes start in a particular state-action pair to ensure coverage and every pair has nonzero probability of being selected as epoch init. This is called "exploring starts". 

### MC Control
- Problem setting: 
	- how can MC estimation be used for control (approx optimal policy)
- Use GPI of policy eval and iteration
	- Policy eval described above and assume $\infty$ episodes to converge to action value function exactly 
	- Policy improvement is just argmax action of action value function (greedy policy of action value function) and by the policy improvement theorem, this guarantees that the updated policy is just as good if not better than the existing policy. 
	- Assumptions made: 
		- episodes have exploring starts (to best converge to true action value function)
		- policy eval has $\infty$ episodes (unrealistic in practice)
- Monte Carlo with Exploring Starts (ES): 
	- get rid of 2nd assumption and do value iteration on episode level - after each epode, observed returns used for policy eval and then policy improved for all states visited in the episode
	- Algorithm:
	  ![[Pasted image 20240915013428.png]]
- Monte Carlo Control **without** ES: 
	- just make agent continue to select actions
	- On-policy methods: 
		- eval or improve policy used to make decisions
		- example: Monte Carlo ES
		- Typically soft (stochastic) where all $\pi(a|s)>0$ and becomes more deterministic
- $\epsilon$-greedy policy: 
	- On-policy method
	- $\epsilon$-greedy policies choose greedy but with proba $\epsilon$ select action at random. Since $\epsilon \in [0, 1]$ and as $\epsilon \rightarrow 1$, the policy becomes more exploratory and less greedy. 
	- nongreedy (exploratory) actions are given minimal probability of selection $\epsilon / |A(s)|$ while greedy action given $1 - \epsilon + \epsilon / |A(s)|$ 
	- This is considered an $\epsilon$-soft policy, which $\pi(a|s) \geq \epsilon/|A(s)|$ for all s, a, for some $\epsilon > 0$. 
	- This solve the hard assumption of exploring starts
	- By the policy improvement thm, any $\epsilon$-greedy policy w.r.t. $q_{\pi}$ is an improvement over any $\epsilon$-soft policy $\pi$. 
	- What we instead move $\epsilon$-soft into the dynamics of the environment itself? 
		- the best one can do in this new env with general policies is the same as the best one could do in the original env with $\epsilon$-soft policies (equivariant)

### Off-policy Prediction via Importance Sampling
- Problem statement: 
	- on-policy $\epsilon$-greedy is a compromise: learns action values not for optimal policy but for near-optimal policy that still explores
	- more straightforward approach is to use 2 policies: target policy (learned and becomes optimal policy), behavior policy (more exploratory and is used to generate behavior)
		- off-policy methods generally have higher variance and slower to converge 
- Simple case (prediction problem): both target $\pi$ and behavior b policies are fixed. 
	- Assumption of Coverage: every action taken under $\pi$ is also taken (at least occasionally) under b: $\pi(a|s) > 0 \rightarrow b(a|s) > 0$
		- follows that b must be stochastic in states where it's not identical to $\pi$ . 
		- $\pi$ can be deterministic (especially important in control applications)
- Importance Sampling: technique for estimating expected values under one distribution given samples from another
	- importance-sampling ratio: weighting returns according to relative proba of their trajectories w.r.t. target/behavior policies. Note that this only depends on policies and sequence, not on MDP
	  $$
	  \rho_{t:T-1} = \Pi_{k=t}^{T-1}\frac{\pi(A_{k}| S_{k})}{b(A_{k}|S_{k})}
	  $$
	- Problem: we want values under target policy but we only have returns $G_{t}$ under behavior policy, which isn't desired expected return. This is reweighted by importance sampling ratio to have correct expected value: 
	  $$
	  \mathbb{E}[p_{t:T-1}G_{t}|S_{t}=s] = v_{\pi}(s)
	  $$
	- Notation: 
		- let time steps be numbered in a way that increases across episode boundaries (i.e. timesteps are unique globally), which allows to us to define $\mathcal{T}(s)$, the set of all time steps in which state s is visit. 
		- Let $T(t)$ denote first time of termination following time t and $G_{t}$ denote return after t up through $T(t)$
		- $\{G_{t}\}_{t\in \mathcal{T}(s)}$ are returns for state s and $\{\rho_{t:T(t)-1}\}_{t\in \mathcal{T}(s)}$ are corresponding importance-sampling ratios. 
		- To get the values, we just have a simple average weighted by the importance sampling ratios called Ordinary importance sampling: 
		  $$
		  V(s) = \frac{\sum\limits_{t\in \mathcal{T}(s)}\rho_{t:T(t)-1}G_{t}}{|\mathcal{T}(s)|}
		  $$
		   - important alternative is weighted importance sampling defined below and 0 when denominator is 0. 
		     $$
		  V(s) = \frac{\sum\limits_{t\in \mathcal{T}(s)}\rho_{t:T(t)-1}G_{t}}{\sum\limits_{t\in \mathcal{T}(s)}\rho_{t:T(t)-1}}
		  $$
		  - Difference between two is the bias-variance tradeoff: 
			  - Ordinary importance sampling is unbiased while Weighted importance sampling is biased (but bias converges towards 0)
			  - Ordinary is unbounded bc variance of ratios is unbounded while weighted estimator is restricted to variance of at most 1 (much lower variance and more useful in practice)


### Incremental Implementation
- Consider the weighted importance sampling case. We have sequence of returns $G_{1}, \dots, G_{n-1}$ starting at the same state and each with corresponding random weight $W_{i}$ as the importance ratio. The update rule is then: 
  $$
  \begin{align}
  V_{n+1}= V_{n}+ \frac{W_{n}}{C_{n}}[G_{n}-V_{n}], n \geq 1 \\
  C_{n+1}= C_{n}+ W_{n+1}, C_{0}=0
  \end{align}
  $$
  where $C_{n}$is the cumsum of weights given first n returns 
  ![[Pasted image 20240915235347.png]]

### Off-policy MC Control
- recall that behavior policy can be unrelated to target policy that's evaluated and improved to eventually be greedy since behavior policy can continue to sample all possible actions for exploration (to solve exploring starts assumption)
  ![[Pasted image 20240916000914.png]]
- Use greedy for $\pi$ and $\epsilon$-soft policy for b with weighted importance sampling. 
- Problem: this method only learns from tails of episodes where all remaining actions are greedy. If nongreedy options are common then learning is slow, especially for states near beginning of long episodes
	- solution here is to incorporate temporal-difference learning

