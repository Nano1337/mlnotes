### Overview: 
- MC Methods don't assume complete knowledge of env (no need for knowledge of dynamics) - require only experience: sample seq of S,A,R with env
- Assumptions: 
	- episodic
	- only on episode completion do value estimates and policy change
	- method based on averaging complete returns

### MC Prediction:
- Recall value of a state is expected return of future states. One way to estimate this is to average returns observed after visits to that state. 
	- As more returns are observed, then average should converge to expected value 
	- Don't bootstrap as previously defined bc estimates for each state are independent (unlike DP which convergence relies on previously cached calcs)
- Types of visits: each occurrence of state $s$ in episode is "visit to s"
	- first-visit MC: 
		- estimates $v_{\pi}(s)$ as average of returns following all visits to s (deduplicate multiple occurrences of s and only keep first visit per episode)
		- Each return is IID estimate of $v_{\pi}(s)$ with finite variance. By law of large numbers, this converges to expected value and mean is unbiased estimate. Standard dev of error is $1/\sqrt{n}$ where n is num returns averaged 
		  ![[Pasted image 20240910110405.png]]
	- every-visit MC: 
		- average returns following all visits to s 
		- estimates converge quadratically to $v_{\pi}(s)$

### Examples
- Blackjack is formulated as episodic finite MDP but state space is massive so infeasible to conduct state sweep for DP. Learn from experience instead. 
- MC methods estimate value of single state independent of number of states (i.e. don't scale with num states) so very cheap to generate many sample episodes starting from states of interest and only averaging returns from those states
	- In other words, can select states that you are seeing returns from, don't have to enumerate sweep across all possible states (many of which can be irrelevant). This allows injection of priors. 

### MC Estimation of Action Values
- if model not available, then very useful to estimate action values too
- state action pair is visited in an episode if ever the state s is visited and action a is taken 
- both first and every visit converge quadratically as num visits to each state-action pair approaches inf
- problem is that many state-action pairs are never visited, especially for deterministic policy (will only observed returns for one of actions from each state). The solution would be estimating value of all actions from each state, not just the one we currently favor
	- Falls under general problem of maintaining exploration
	- Concrete solution is to specify episodes start in a particular state-action pair to ensure coverage and every pair has nonzero probability of being selected as epoch init. This is called "exploring starts". 

### MC Control
- Problem setting: 
	- how can MC estimation be used for control (approx optimal policy)
- Use GPI of policy eval and iteration
	- Policy eval described above and assume $\infty$ episodes to converge to action value function exactly 
	- Policy improvement is just argmax action of action value function (greedy policy of action value function) and by the policy improvement theorem, this guarantees that the updated policy is just as good if not better than the existing policy. 
	- Assumptions made: 
		- episodes have exploring starts (to best converge to true action value function)
		- policy eval has $\infty$ episodes (unrealistic in practice)
- Monte Carlo with Exploring Starts (ES): 
	- get rid of 2nd assumption and do value iteration on episode level - after each epode, observed returns used for policy eval and then policy improved for all states visited in the episode
	- Algorithm:
	  ![[Pasted image 20240915013428.png]]
- Monte Carlo Control **without** ES: 
	- just make agent continue to select actions
	- On-policy methods: 
		- eval or improve policy used to make decisions
		- example: Monte Carlo ES