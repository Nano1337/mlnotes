
### Overview
![[Pasted image 20240829104926.png]]
- Policy: mapping from perceived states to actions to be taken in a particular state. RL agent uses policy to select actions given current state
- Reward: defines the goal, formulation of optimization given from this signal to update policy
- Value function: expectation of total reward an agent will accum over future, starting in that state
	- Value at a certain state is weighted sum of rewards in all (accessible) future states. Rewards in future states are time-discounted $\gamma \in [0,1
	- Action choices are made based on value judgements. Thus, pick actions that bring about states of highest value (rather than greedy reward)
### Trade-offs
Exploitation: 
- Select action that leads to state with highest immediate value (but not always most optimal long-term move). Also known as greedy
Exploration: 
- Select a random action to experience states that otherwise aren't considered

### Temporal Difference Learning
$$
V(S_{t}) = V(S_{t}) + \alpha[V(S_{t+1}-V(S_{t})]
$$
where $\alpha$ is learning rate, $S_{t}$ is state before greedy action and $S_{t+1}$ is state after greedy action. This update rule is an example of a temporal-different learning method bc its changes are based on a difference between estimates that 2 successive times. If $\alpha$ is properly reduced by some scheduler, then this properly converges to optimal policy (at least for tic tac toe). 
- If not, the policy is suboptimal but still plays well against opponents that slowly change their way of playing (within distribution of policy)


### Evaluation of a Policy
- **Evolutionary Method:**
	- holds policy fixed and simulate many games, freq of wins is unbiased estimate of P(win; policy). Only problem is that this is a sparse reward signal, ignoring what happens *during* the game. 
- **Value function methods**: 
	- allows evaluate of individual states

**Scenarios to think about**: 
- Self-play: 
	- Would result in better generalization and robustness that eventually converges to Nash equilibrium
- Symmetries: 
	- Would simplify state space and improve generalization for a particular state space. This would also lead to more samples per state space. 
- Greedy Play: 
	- pure exploitation, may get stuck in local minima solutions
- Exploration play: 
	- not learning from exploration = greedy play
	- learning from exploration gives values from wider range of experience, giving a more accurate estimate of value at each state
