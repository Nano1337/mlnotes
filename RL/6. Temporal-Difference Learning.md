
### TD Prediction
- Same as MC, use experience for prediction problem
- Every-visit MC for nonstationary env: 
  $$
  V(S_{t}) \leftarrow V(S_{t})+\alpha[G_{t}-V(S_{t})]
  $$
  where $G_{t}$ is return after time t and $\alpha$ is constant step-size param. This is called constant-$\alpha$ MC. As per MC methods, we have to wait until end of episode to run update (bc $G_{t}$ only known then) -> TD methods only wait until next time step to update.
- Simplest TD Update: 
  $$
  V(S_{t}) \leftarrow V(S_{t}) + \alpha[R_{t+1} + \gamma V(S_{t+1}) - V(S_{t})]
  $$
  note to self, this looks very close to original value function part with r + $\gamma$ v_{s+1}. 
	- the target for MC is $G_{t}$ and for TD is $R_{t+1}+\gamma V(S_{t+1})$ . 
	- This is called TD(0) or one-step TD
	  ![[Pasted image 20240916001920.png]]
		- bc TD(0) bases its update from existing estimate (like DP), it's a bootstrapping method
- TD combines sampling of MC (estimate of returns) with bootstrapping of DP (estimate V instead of true $v_{\pi}$ )
- TD error ($\delta_{t}$) is stuff inside square brackets bc it's the diff between estimated value of $S_{t}$ and the better estimate $R_{t+1}+\gamma V(S_{t+1})$
	- TD error only found at next state so $\delta_{t}$ is only available at time t+1. The MC error can be written as sum of TD errors if V doesn't change during episode (like in MC methods)
	  $G_{t}-V(S_{t}) = \sum\limits_{k=t}^{T-1}\gamma^{k-t}\delta_{k}$

### Advantages of TD Prediction Methods
- better than DP bc don't require dynamics of env 
- better than MC methods bc online, fully incremental
- for fixed policy $\pi$ , TD(0) is proven to converge to $v_{\pi}$ in the mean for constant step-size param if small enough

### Optimality of TD(0)
- Given finite experience and V, increments are computed but value function are only changed once, by sum of all increments and this process is repeated until convergence. This is called batch updating. 
	- under batch updating, TD(0) converges deterministically to single solution independent of $\alpha$ (small enough). constant-$\alpha$ also converges but to a different number. 
	- Batch MC methods find estimates that minimize MSE on train set (overfits) but batch TD(0) find estimates that would be exactly correct for MLE of Markov process.
	- Certainty-equivalence estimate: given transition model, can compute estimate of value function that would be exactly correct if model were exactly correct. Batch TD(0) converges to this. 

### Sarsa: On-policy TD Control
- learn action-value function instead of state-value function: 
  $$
  Q(S_{t},A_{t}) \leftarrow Q(S_{t},A_{t}) + \alpha [R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_{t},A_{t})]
  $$
- Name comes from quintuple of events $(S_{t},A_{t},R_{t+1},S_{t+1},A_{t+1})$ 
  ![[Pasted image 20240917182726.png]]

### Q-Learning: Off-policy TD Control 
- learned action-value function Q directly approximates $q_{*}$ independent of policy being followed:
  $$
  Q(S_{t},A_{t}) \leftarrow Q(S_{t},A_{t}) + \alpha[R_{t+1}+\gamma \max_{a}Q(S_{t+1},a) - Q(S_{t},A_{t})]
  $$
  ![[Pasted image 20240917183241.png]]
  
### Expected Sarsa
- Q-learning but instead of taking the max of next state-action pairs, it uses the expected value: 
  $$
  Q(S_{t},A_{t}) \leftarrow Q(S_{t},A_{t}) + \alpha[R_{t+1} + \gamma \sum\limits_{a}\pi(a|S_{t+1})Q(S_{t+1},a)-Q(S_{t},A_{t})]
  $$
- Given the next state $S_{t+1}$, this algo moves deterministically n the same direction as Sarsa moves in expectation 
- It can be on-policy but in general can use policy different from target to generate behavior = off policy. 
	- If $\pi$ is greedy policy while behavior is exploratory = Q-learning

### Maximization Bias and Double Learning
- Problem: Q-learning has target greedy policy and Sarsa often is $\epsilon$-greedy - both involve max operation. Maximum overestimated values is used as an estimate of the maximum value, which can lead to significant positive bias = maximization bias
- Example: 
  - Let (right,A) be terminal state with reward 0 and (left,B) have terminal state with reward from normal distribution with mean -0.1 and var 1.0, which means that any trajectory starting with left has expected return of -0.1 and left action is always a mistake. However, Q-learning with $\epsilon$-greedy initially strongly favors left and at convergence takes left action 5% more than optimal. 
    ![[Pasted image 20240917190224.png]]
  - Solution: factorize the $Q(s,a) \rightarrow Q_{2}(argmax_{a}Q_{1}(a))$. In this case, $Q_{1}$ determines the maximizing action and $Q_{2}$ provides value estimate. This is double learning. 
    - although two estimates are learned, only one estimate is updated on each play so this doubles memory requirements (but doesn't increase computation)
  - Double Q-learning: 
    $$
    Q_{1}(S_{t},A_{t}) \leftarrow Q_{1}(S_{t},A_{t}) + \alpha [R_{t+1} + \gamma Q_{2}(S_{t+1},argmax_{a}Q_{1}(S_{t+1},a))-Q_{1}(S_{t},A_{t})]
    $$
    - This is just flipped for the $Q_{2}$ update. For each timestep, randomly with equal probability, choose one of the two Q to update and this is treated symmetrically. 
    - Example: $\epsilon$-greedy policy for Double Q-learning based on average of action-value estimates
      ![[Pasted image 20240917192216.png]]
      

### Afterstates
- For games like tic-tac-toe that evaluates board positions after the agent has made its move, this is known as an afterstate and value function is called an afterstate value function. 
- This is particularly important in adversarial games where we don't have knowledge of how opponent will reply if we only look at current state and actions. Afterstates allows us to incorporate adversarial knowledge and produce more efficient learning method
- Example: 
	- move pairs in TTT can produce the same board, which should have the same value no matter how we arrived there. 
	  ![[Pasted image 20240917192836.png]]
	  - Traditional action value functions would have to estimate each resulting state separately and be inefficient